---
title: "Saying ’Hi!’ Is Not Enough:Mining Inputs for Effective Test Generation"
date: 2022-01-14
description: "Identification of near-duplicate document in web crawling is a very difficult problem. Such documents, near-duplicate, are similar in term of content but differ in a small portion such as advertisements. Such page should crawled only one time a crawler, because different information is no relevant for web search. The most import issue is to scale on billion web pages. Manku et al. propose to use the hash function simhash to identify near-duplicate web pages and a technique for solving the Hamming Distance Problem (see below for more information). Simhash maps large vectors to small fingerprints. It is applied as follows: split the web page into weighted features. These weighted features are a large vector, and it is convert into a small fingerprint by simhash. This conversion is done in three steps: First, it initializes all dimensions of a vector (of the size of the fingerprint) with zero; Second, it computes the hash of each features, unique to each feature; Third, for each hash, if the i-th bits is equals to one, it increments of the weight of the features, the i-th dimension of the vector, and decrement in case the i-th is equals to zero. The sign of each dimension determine the value of each bits of the fingerprint. Simhash has two main advantages: the fingerprint of a document is a hash of its features, and similar document have similar hash. The latter property lead to the Hamming Distance Problem. In fact, how to find in the set of computed hashes, crawled web page that differ, of 3 bits for instance, in their hash? There are two scenarios for the Hamming Distance Problem. In the online version, we have to find fingerprints that differ from the queried in milliseconds. In the batch version, we have a lot of queries, e.g. one million, and we have to solve them in one hundred seconds. In both case, they applied the same approach: They use a sorted table. Taking the d most significant bits, it brings the table a simple counter, and the less significant bits are just random. Now, take a d' such that d' - d is small. We know that the table is sorted, so a single probe is sufficient to find all fingerprints that match in d' most significant bit-positions. For each of them, we can easily checks if they differs from the original fingerprint in at most k bit-positions.They experimented their approach on eight billions web pages. (Did not find explicit results about performance)."
draft: false
link: "http://dl.acm.org/citation.cfm?id=1242592"
---


Detecting near-duplicates for web crawling.



