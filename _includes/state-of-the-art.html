<section class="section projects-section">
    <h2 class="section-title"><i class="fa fa-file-pdf-o"></i>State of the Art</h2>
    <div class="intro">
        <p>Quick abstract of some papers</p>
    </div><!--//intro-->
    <!-- <div class="item">
        <span class="project-title"><h4>Alleviating Patch Overfitting with Automatic Test Generation: A Study of Feasibility and Effectiveness for Nopol</h4></span>
        <p>Authors aim at alleviating patch overfitting in test based repair techniques.They define I_bug, the buggy
            input domain; I_patch, the input domain
            on which the patch has an impact and I_correct, the remaining input domain, for which the behavior of the
            program under repair is correct.
            The perfect patch generated by a test based repair technique would be a patch with an input domain equals to
            the buggy input domain, <i>i.e. </i>
            I_bug = I_patch. The overfitting problem is the fact there are two potential in a patch generated by a
            test bases repair technique: 1) the patch does not fix totally the bug; 2) the patch fix the bug but break
            some code
            in another part of the program, <i>i.e. </i> introduce a regression. They define three kinds of
            over-fitting:</p>
        <ul>
            <li>A-overfitting: the patch does not fix totally the bug.</li>
            <li>B-overfitting: the patch fix the bug but introduce a regression.</li>
            <li>AB-overfitting: the patch does not fix totally the bug and introduce a regression.</li>
        </ul>
        <p>Authors devise a techniques, named UnsatGuided that aim at alleviating those three kinds of overfitting. They propose to use
        automatic generated test cases to guide the synthesize of the patch. The intuition is that is a generated test has a contradiction
        with the manually written tests, this generated test encode the bug and expose it. The bug-exposing test, that is how authors call such test,
        is used to enforce constraints in the repair techniques.</p>
        <p>To evaluate UnsatGuided, they use 224 bugs from the Defects4j benchmark. For the repair technique they use Nopol and generated test cases with Evosuite.
            The experiments results with following observations: 1) Overfitting is common in patch generated by test based repair techniques for real buf in real world application;
            2) UnsatGuided alleviate successfully the overfitting in some cases; 3) the time overhead introduce by UnsatGuided is acceptable in most cases.
        </p>
    </div>--><!--//item-->

    <div class="item">
        <span id="ref1" class="project-title"><h4>Constraint-Based Automatic Test Data Generation<a
                href="http://dl.acm.org/citation.cfm?id=126269#"><i class="fa fa-link"
                                                                    aria-hidden="true"></i></a></h4></span>
        <p>
            DeMillo <i>et al.</i> proposed a first attempt to automatically generate test to meet the fault-based
            testing
            criteria. The fault-based testing criteria is to use mutation analysis, <i>i.e. </i> inject seeded fault
            into a program, then exercise the tests. If the tests fails, it kills the mutants and detects the fault. If
            not, there is a potential lack of test data input in the test suite. This lead the testers to produce this
            new test data input. However, this task is tedious and time-consumption, that why, authors proposed a
            technique to generate them.
        </p>
        <p>
            Their techniques is base on a representation algebraic constraints to kill mutants, and then use this
            representation to generate data to kill it. They called their techniques <i>Constraint-based Testing</i>
            (CBT). In order to kill mutants, Generated test data input must makes a difference in the behavior of the
            mutant. They define necessity constraints templates based on mutation operators used, in Mothra. They
            overcome mutant equivalent program, they add constraints on predicate to force mutant programs to have
            different value than original program.
        </p>
        <p>
            In their techniques, constraints are composed by algebraic expression, <i>i.e.</i> variables, parentheses
            and programming language operators. A constraint is a pair of algebraic expression related with one
            conditional operator. A clause is a conjunctive or disjunctive list of constraints. A constraint is a clause
            that represent one test case.
        </p>
        <p>
            The global workflow of their approach is to successively substitute value for a variable in constraints that
            remains consistent with the rest of the clauses. At the begin, all the values are assigned to a domain that
            contains all the possible values. The reduction aims at eliminate expression in the constraints, and reduce
            the domain of each values by substitute variables by values. At each steps they take the variables with the
            smallest domain.
        </p>
        <p>
            They implemented CBT for Fortran 77 programs in a tool named Godzilla. They use the widely use triangle
            classification program TRITYP. They compare generated test by Godzilla to manually written-test cases (in 30
            hours). Godzilla achieve a better mutation score: 99% against 95 for the manually-written test.
        </p>
    </div>

    <div class="item">
        <span id="ref2" class="project-title"><h4>An Approach to Test Data Generation for Killing Multiple Mutants <a
                href="http://ieeexplore.ieee.org/document/4021328/"><i class="fa fa-link"
                                                                       aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Mutation analysis is an approximation of fault detection potential of a test suite. Liu <i>et al.</i> aim at
            generating a "good" test suite. To do so, they devise a techniques that generate test data that kills
            multiple mutants.
            The idea is that for multiple mutant that are the same location, the constraints to reach them are the same.
            They improve Constraint Based Testing (CBT) by introduce their Improved Iterative Relaxation Method IIRM.
        </p>
        <p>
            The approach is based on the observation that mutants at the same location (same-location mutants) have the
            same reachability conditions. The approach use pair-wise test data generation techniques rather than
            symbolic execution. The approach is done in three steps: 1) collect reachability condition during generation
            of mutants; 2) combine same-location mutants reachability conditions; 3) program path formations.
        </p>
        <p>
            To express reachability conditions, Authors use branch predicates, instead of execution paths in CBT. There
            are two main advantages to use branch predicates: 1) there is no need to figure out a path expression to
            represent all the branch predicates; 2) it is no more necessary to figure out all executions path from the
            starting points. They compute reachability condition of each mutant while traversing the parse tree of the
            original program. They push on a stack the branch predicate when the first statement of one block is met,
            and they pop the top element when the last one is met.
        </p>
        <p>
            To combine reachability condition of same-locations mutants, they distinguish two categories of locations:
            1) Several mutation operator can be applied to the location; 2) location with only on mutation operator that
            produce multiple mutants. For the former case, there is no contradiction in there used mutation operators,
            they combine them into one condition by conjunction. For the latter case, there is two mutation operators:
            arithmetic and relational. For arithmetic, there is no contradiction so they combine reachability condition
            by conjunction. For relationel, there is contradiction. There reduce the combination to two conjunctions.
        </p>
        <p>
            Last but not least, they generate the desired test data by using transforming the combined necessity into
            branch predicates. They use existing path-wise test data generation to obtain the desired data, <i>i.e.
            covering the desired path.</i>.
        </p>
        <p>
            They evaluate their approach on 5 JAVA programs, ranging from 5 to 21 statements. They applied 5 most
            frequently mutation operators of their tools, named JUTO. They show that with their approach, they need
            fewer (20% to 40%) less test data to have almost the same mutation score. Less data generated means less
            time consumption.
        </p>
    </div>

    <div class="item">
        <span id="ref3" class="project-title"><h4>The care and feeding of wild-caught mutants <a
                href="http://dl.acm.org/citation.cfm?doid=3106237.3106280"><i class="fa fa-link"
                                                                              aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Brown <i>et al.</i> highlight the fact that mutants (changes in source code) in mutation analysis do not
            necessarily reflect errors made by developers. They propose an approach named <b>wild-caught mutants</b> to
            generate mutants that reflect errors introduce by developers instead of classic changes.
        </p>

        <p>
            To create those new mutation operators, they use revision of software. They take the reverse (backward) of
            what is likely to be a correction in the revision history. They have 4 research axes: 1) Does the technique
            find existing mutations operators? 2) Does the technique find new mutation operations? 3) In what way new
            mutation operators are different from existing? 4) Does the forward generation is different from the
            backward generation?
        </p>

        <p>
            Their approach is devised with two tools: <i>mutgen</i> and <i>mutins</i>. The former is responsible to
            extract new mutation operators from a set of code diffs. Then, the latter apply this new mutation operators
            to a new code base. Both tools also use a language definition.
        </p>

        <p>
            <i>Mutgen</i> extract candidate mutation operators by analyzing diff between two version of program. It is
            isolating changes into small block that are contiguous. Each of this block are treated separately. To
            identify a candidate, changes must not require any synthesise to run its "after" state into "before"
            state. After the extraction they filter the candidate using diverse criteria: length of the sequence,
            candidate that contains repeated specific characters such as "*" or "/", unbalanced bracket... During this
            process, they also collect identifier-shift, which are mutation operators that solely change an identifier
            by another.
        </p>
        <p>
            <i>Mutins</i> attempts to insert mutation operator into another codebase. To do so, it try to match the
            operators to the token stream. Then, among the matches, it insert the mutants into the code (selected
            randomly
            or specified by the user).
        </p>

        <p>
            To evaluate the approach, they use project on github and their history. They used the full revision of the
            top 50 project, order by the number of forks. They use <i>Mutgen</i> on the 50 projects to create the set of
            mutation operators.
        </p>

        <p>
            For <i>Mutins</i>, they targeted the program Space(Filippos I. Vokolos and Phyllis G. Frankl. 1998.
            Empirical Evaluation of the Textual Differencing Regression Testing Technique.). For this program, they
            generated 5,000 100-cases test-suite. They then compute the number of compiled mutants, and the number of
            mutant that have been killed.
        </p>

        <p>
            Their approach allows to reproduce four common operators. They identified three stronger mutation operators
            and two new mutation operators. The wild-caught mutants are as hard to detect that existing (0.81
            sampl-based mutation-detection ratio against 0.75 for classical operator). The compatibility is lower 14%
            against 92% reported by Andrews <i>et al.</i> (J. H. Andrews, L. C. Briand, and Y. Labiche. 2005. Is
            Mutation an Appropriate Tool for Testing Experiments?).
        </p>

        <p>
            The difference between backward and forward generation is in the number of patches found by the techniques:
            6,359 for forward and 5.860 for backward. For the 38 bugs of Space, 7 faults would be reintroduced by
            backward patches, against only one by both. Five other bugs could be reproduce using the identifier-shift.
        </p>
    </div>

    <div class="item">
        <span id="ref4" class="project-title"><h4>Saying ’Hi!’ Is Not Enough:Mining Inputs for Effective Test Generation<a
                href="https://www.conference-publishing.com/list.php?Event=ASE17"><i class="fa fa-link"
                                                                                     aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Generators of tests fails to reproduce a majority of bugs (see [<a href="#ref5">5</a>]). This is explained
            by the fact that generator tends at using wrong inputs. For instance, most of generator use random generated
            values or values from a predefined pool. But this techniques do not obtain good results because programs use
            domain-specific data formats. Usually, generated inputs tends to not trigger deep codes because they do not
            pass the sanity-check from the domain. Toffola <i>et al. </i> presents <b>TestMiner</b>, a novel techniques
            to address this issue. Their technique is to exploit the knowledge inside existing tests (amplification) to
            generate proper input. This approach is devised into two steps: 1) It extracts existing literals from tests
            and index them for a quick retrieval; 2) the test generator queries this index for a given method call.
        </p>

        <p>
            The approach first analyze statically the program. It extracts from this analysis input values associated
            with a context. A context might be anything that can be represented as a bag of words. In their approach,
            they choose to use the full qualified name of methods. For each literals, the analysis returns a set of call
            site tuples <i>Sc</i>. A call site tuples is three elements: the qualified names of the type that defines
            the method called, the name of the method called and the literals used as input. From Sc, they build a bag
            of word that summarizes the context. They put context into an HashMap. They assign a specific weigh to each
            word of bags, because some of them are more relevant than others. For instance, we have the following
            context: ({org, sql, parser}; "SELECT x FROM y"). The word <i>sql</i> contains more information about the
            context than the word <i>org</i>, which is a common package. To leverage this, they compute the <i>term
            frequency-inverse document frequency</i> for each words. They use a specific Hash function for the indexing:
            Simhashing, which allows <b>TestMiner</b> to assign similar hash to similar values.
        </p>

        <p>
            <b>TestMiner</b> retrieves string values for a test generator. First, it assign a weight to the query. The
            used weight function is prioritizing uncommon words. The search algorithm used is the presented by Manku <i>
            et al. </i> [<a href="#ref6">6</a>]. Named searchSimhash, the algorithm returns a mapping between string and
            a list of integers. The algorithm selects multiple indices. This is allow to have all indices of string
            values that differ of a given number of bits from the querried hash. The more this value is high, the more
            the algorithm returns values.
        </p>

        <p>
            They implement <b>TestMiner</b> in Randoop, a state-of-the-art generator of test in Java [<a
                href="#ref7">7</a>]. Their implementation is a web server, that allows to modify only one hundred lines
            of code in Randoop. They learned data from 3,601 Java projects from the Maven Central Repository, which
            results with 263,276 string values in 37,821 different contexts. They used 40 classes from 18 Java projects
            to evaluate the effectiveness of tests generated with the help of <b>TestMiner</b>. Then, they generated 10
            test suites using only Randoop and Random enhanced with <b>TestMiner</b>, and compare the branch coverage
            obtained, computed with JaCoCo.
        </p>

        <p>
            It results that using <b>TestMiner</b> with Randoop increase the coverage for thirty classes and decreases
            it for two classes. On average, the relative improvement is 21%.
        </p>

        <p>
            <b>TestMiner</b> provides results examples that fit real contract such as IBAN, SQL, Network address, or
            E-mail. <b>TestMiner</b> has an overhead of 55% in time consumption.
        </p>

    </div>


    <div id="ref5" class="item">
        <span class="project-title"><h4>Do automatically generated unit tests find real faults? an empirical study of effectiveness and challenges<a
                href="http://dl.acm.org/citation.cfm?id=2916250"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Automatic generated unit tests performs well according to the code coverage. However, covering code does not
            imply to detect fault. This is why, using mutation analysis to measure the ability of tests to detect faults
            is more appropriate. But killing a lot of mutants, that are seeded and artificial faults, do not necessarily
            means that test are able to detect fault done by developers. This is why, Shamshiri <i>et al. </i> studied
            empirically the effectiveness of generated tests to detect real faults.
        </p>

        <p>
            To do this, they use three state-of-the-art generators of tests for Java: <b>Randoop</b>, <b>EvoSuite</b>,
            and <b>Agitar-One</b>. They used Defects4j, a well-known dataset of real bugs. Their contributions show a
            large-scale experiments, a detailed analysis of the performance for generator of tests, and new paths of
            worthwhile investigation for the improvement of tests generations.
        </p>

        <p>
            Their experimental procedure is as follows: For the test generation, <b>Randoop</b> and <b>EvoSuite</b> are
            not deterministic. To take in account this randomness, they generate ten test suites for each tools. For <b>Agitar-One</b>,
            it needs manual effort to generate tests. They generate only one test suite. Then, they removed all
            non-compilable tests. They also remove flaky tests, <i>i.e.</i> test that are unstable, by running them
            fives time. In the test suites, some tests might be false positive, <i>i.e.</i> failing but not because of
            the fault. To leverage this issue, they compare the error message with the original one, <i>i.e.</i> the one
            that are provided by <b>defects4j</b>. If the error message, the exception stack trace or the failing
            assertion's message, is the same than the generated test's message, then the test is failing because of the
            fault, otherwise it is another reason, such as calling calling private method using reflection. They then,
            run tests to detect the failure. During this execution, they compute the statement coverage to study how it
            is related to fault detection.
        </p>

        <p>
            <b>EvoSuite</b> and <b>Randoop</b> generated 3.4% and 8.3% non-compilable test suites on average. Moreover,
            on average, 21% of <b>Randoop</b>’s tests were flaky, and 46% of <b>AgitarOne</b>’s failing tests were false
            positives. Automated test generation tools found 55.7% of the bugs considered, but no tool alone found more
            than 40.6%.
        </p>
    </div>


    <div id="ref6" class="item">
        <span class="project-title"><h4>Detecting near-duplicates for web crawling.<a
                href="http://dl.acm.org/citation.cfm?id=1242592"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Identification of near-duplicate document in web crawling is a very difficult problem. Such documents,
            near-duplicate, are similar in term of content but differ in a small portion such as advertisements. Such
            page should crawled only one time a crawler, because different information is no relevant for web search.
            The most import issue is to scale on billion web pages. Manku <i>et al.</i> propose to use the hash function
            <b>simhash</b> to identify near-duplicate web pages and a technique for solving the Hamming Distance Problem
            (see below for more information).
        </p>

        <p>
            <b>Simhash</b> maps large vectors to small fingerprints. It is applied as follows: split the web page into
            weighted features. These weighted features are a large vector, and it is convert into a small fingerprint by
            <b>simhash</b>. This conversion is done in three steps: First, it initializes all dimensions of a vector (of
            the size of the fingerprint) with zero; Second, it computes the hash of each features, unique to each
            feature; Third, for each hash, if the i-th bits is equals to one, it increments of the weight of the
            features, the i-th dimension of the vector, and decrement in case the i-th is equals to zero. The sign of
            each dimension determine the value of each bits of the fingerprint.
        </p>

        <p>
            <b>Simhash</b> has two main advantages: the fingerprint of a document is a hash of its features, and similar
            document have similar hash. The latter property lead to the Hamming Distance Problem. In fact, how to find
            in the set of computed hashes, crawled web page that differ, of 3 bits for instance, in their hash?
        </p>

        <p>
            There are two scenarios for the Hamming Distance Problem. In the online version, we have to find
            fingerprints that differ from the queried in milliseconds. In the batch version, we have a lot of queries,
            <i>e.g. </i> one million, and we have to solve them in one hundred seconds. In both case, they applied the
            same approach: They use a sorted table. Taking the <i>d</i> most significant bits, it brings the table a
            simple counter, and the less significant bits are just random. Now, take a <i>d'</i> such that <i>d' - d</i>
            is small. We know that the table is sorted, so a single probe is sufficient to find all fingerprints that
            match in <i>d'</i> most significant bit-positions. For each of them, we can easily checks if they differs
            from the original fingerprint in at most k bit-positions.
        </p>


        <p>
            They experimented their approach on eight billions web pages. (Did not find explicit results about
            performance).
        </p>

    </div>

    <div id="ref7" class="item">
        <span class="project-title"><h4>Feedback-directed random test generation<a
                href="http://dl.acm.org/citation.cfm?id=1248841"><i class="fa fa-link"
                                                                    aria-hidden="true"></i></a></h4>
        </span>
        <p>

        </p>
    </div>

    <div id="ref8" class="item">
        <span class="project-title"><h4>Is mutation an appropriate tool for testing experiments?<a
                href="http://dl.acm.org/citation.cfm?id=1062530"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
    </div>

    <div id="ref9" class="item">
        <span class="project-title"><h4>Similarity estimation techniques from rounding algorithms.<a
                href="http://dl.acm.org/citation.cfm?id=509965"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
    </div>

    <div id="ref10" class="item">
        <span class="project-title"><h4>Evaluating the “Small Scope Hypothesis”<a
                href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.63.9997"><i class="fa fa-link"
                                                                                          aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Rather than testing fewer test inputs inside a large scope, the "small scope hypothesis" argues that testing
            exhaustively test input within a small scope would be more effective[<a href="#ref12">12</a>]. Andoni <i>et
            al. </i> evaluates this hypothesis using code coverage and mutation analysis on several benchmark in Java.
            If the hypothesis holds, it means that the generation of test inputs should be done exhaustively within a
            small scope rather than a large scope. In fact, enumerate all possible cases in a large scope is not
            practicable.
        </p>

        <p>
            They use the "small scope hypothesis" in the context of generating test input, for Java programs. In Java,
            test inputs are constructed from objects and literals, on which methods are invoked. Andoni <i> et al.</i>
            aim at generate all possible test inputs within a small, but sufficient, scope. To do this, they devise a
            tool names <b>Korat</b>.
        </p>

        <p>
            <b>Korat</b> is a technique that generate input from a given predicate and a bound for them. The predicate
            must be written developers, and describe acceptable inputs. In addition to this, developers must build a <b>Finitization</b>,
            which a specific (Java) object from <b>Korat</b>. In this object, the developer specify the size limit of
            each inputs, <i>i.e. </i> the number of objects for each classes that compose the test inputs. This <b>Finitization</b>
            creates the state space explorable by <b>Korat</b>, which is a "small scope". This scope is defined by the
            <i>class domain</i> and the <i>field domain</i>, which are respectively the set of objects from one class
            and the set of values for each fields.
        </p>

        <p>
            <b>Korat</b> then generate test inputs as follow: it uses a vector of indices. Each index are index of
            values inside <i>class domain</i> and the <i>field domain</i>. This vector is initialized with all zeros.
            Then, it call the predicate using this first values. During, this call, <b>Korat</b> monitors access of
            values inside the vector. This allows to prune the exploration, by only change accessed values during the
            predicate call.
        </p>w

        <p>
            They evaluated <b>Korat</b> by studying the impact of the size of the scope on the code coverage and the
            mutation score on 9 Java classes.
        </p>

        <p>
            The main cons of this technique is that require that developers write manually the predicate and the <b>Finitization</b>.
        </p>
    </div>

    <div id="ref11" class="item">
        <span class="project-title"><h4>Finding bugs with a constraint solver<a
                href="https://dl.acm.org/citation.cfm?id=347636.383378"><i class="fa fa-link"
                                                                           aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Jackson <i>et al.</i> devised a method to find bug in a program. This method is able to provide a
            counterexample that encode the bug. Their approach use Alloy, a specification language. They also ensure
            that their techniques does not produce false positive, by compromising the completeness.
        </p>

        <p></p>
    </div>

    <div id="ref12" class="item">
        <span class="project-title"><h4>Elements of Style: Analyzing a Software Design Feature with a Counterexample Detector<a
                href="https://dl.acm.org/citation.cfm?id=235738"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
        <p>
            TODO
        </p>
    </div>

    <div id="ref13" class="item">
        <span class="project-title"><h4>Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation<a
                href="http://arxiv.org/abs/1711.01661"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Groce <i>et al.</i> present a new approach that allows developers to compute a "<b>provenance</b>" or a "<b>pseudo-provenance</b>"
            of automatically generated test cases. By "<b>provenance</b> they mean the seed test case that allowed the
            considered automated technique of test generation to construct this test. Their approach allows also the
            impact of seeded tests on future tests.
        </p>

        <p>
            When the generation of test is done by adding statements, or modifying some literals for instance, the
            computation of its "<b>provenance</b>" is easy. By comparing bodies, one can retrieve this information. But
            in the case that the generation is mixin up existing test cases to build new ones, the provenance
            information is destroyed during the process.
        </p>

        <p>
            To do this, they split the generated test case, the one that the "<b>provenance</b>" has been (partially)
            destroyed, into components. Then, they compute all positions possible of all component in all seed tests
            cases.
        </p>
    </div>

    <div id="ref14" class="item">
        <span class="project-title"><h4><a
                href="https://dl.acm.org/citation.cfm?id=154246"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>
        <p>
            Hamlet <i>et al.</i> proposes a new kind of reliability analysis. A method is better if it amplifies the
            reliability measurements.
        </p>

        <p>
            They state that the reliability cannot be measured directly because of the time that a such approach would
            take. To establish a mean time to
            failure (MTTF) with 90% confidence is at least twice that MTTF. There are about 107 sec in a work-year.
        </p>

        <p>
            TODO
        </p>

    </div>

    <div id="ref15" class="item">
        <span class="project-title"><h4>Shadow Symbolic Execution with Java PathFinder</h4><a
                href="https://dl.acm.org/citation.cfm?id=3149492"><i class="fa fa-link" aria-hidden="true"></i></a></h4>
        </span>

        <!-- SUMMARY -->
        <p>The authors aim at detecting regression bug. To This, the authors apply the <b>shadow symbolic execution</b>,
            original from <i>Palikevera</i> with the <i>shadow<sub>KLEE</sub></i> for C program, on JAVA program. Shadow
            symbolic execution generate test inputs that trigger the new program behavior. To this, it uses a merged
            version of the both version of the same program, <i>i.e.</i> the previous version, so called old, and the
            changed version, called new. This done by instrumenting the code with annotation as method calls
            <b>change()</b>. The method <b>change()</b> takes two inputs: the old statement and the new one. Then, it
            uses two steps: 1) the concoic phase. This step collects divergence points, <i>i.e.</i> conditional where
            the old version and the new version does not take the same branch, <i>e.g. for a given conditional, the old
                version takes the then branch and the new version takes the else branch</i>. Then the bounded symbolic
            execution, <i>a.k.a.</i> <b>BSE</b>.
        </p>

        <!-- EVALUATION -->
        <p>

        </p>

    </div>

    <div class="item">
        <span class="project-title"><h4>Are mutants a valid substitute for real faults in software testing?<a
                href="http://dl.acm.org/citation.cfm?doid=2635868.2635929"><i class="fa fa-link"
                                                                              aria-hidden="true"></i></a></h4>
        </span>
    </div>

    <div class="item">
        <span class="project-title"><h4>Does choice of mutation tool matter?<a
                href="http://link.springer.com/10.1007/s11219-016-9317-7"><i class="fa fa-link"
                                                                             aria-hidden="true"></i></a></h4>
        </span>
    </div>

    <div class="item">
        <span class="project-title"><h4>VART: A Tool for the Automatic Detection of Regression Faults.<a
                href="http://arxiv.org/abs/1708.02052"><i class="fa fa-link"
                                                          aria-hidden="true"></i></a></h4>
        </span>
    </div>

    <div class="item">
        <span class="project-title"><h4>BDCI: Behavioral Driven Conflict Identification.<a
                href="http://arxiv.org/abs/1708.01650"><i class="fa fa-link"
                                                          aria-hidden="true"></i></a></h4>
        </span>
    </div>


</section><!--//section-->